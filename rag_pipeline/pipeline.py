"""High-level orchestration of the academic RAG pipeline."""

from __future__ import annotations

from pathlib import Path
from typing import Iterable, List, Sequence

from .config import PipelineConfig
from .embeddings import EmbeddingGenerator
from .ingestion import load_document
from .llm import LLMClient, build_pipeline_response
from .preprocess import preprocess_document
from .prompt import build_prompt
from .retrieval import HybridRetriever
from .schemas import PipelineResponse, RetrievedChunk, Segment
from .segment import DocumentSegmenter
from .vector_store import AcademicVectorStore


class RAGPipeline:
    """Coordinates ingestion, retrieval, and generation steps."""

    def __init__(
        self,
        config: PipelineConfig | None = None,
        embedding_generator: EmbeddingGenerator | None = None,
        llm_client: LLMClient | None = None,
        segmenter: DocumentSegmenter | None = None,
    ) -> None:
        self.config = config or PipelineConfig()
        self.embedding_generator = embedding_generator or EmbeddingGenerator(self.config.embedding)
        embedding_function = getattr(self.embedding_generator, "_client", None)
        self.vector_store = AcademicVectorStore(self.config.vector_store, embedding_function=embedding_function)
        self.segmenter = segmenter or DocumentSegmenter(self.config.segmenter)
        self.llm = llm_client or LLMClient()
        self.retriever = HybridRetriever(self.config.retrieval, self.vector_store, self.embedding_generator)

    def _ingest_single(self, path: str | Path) -> List[Segment]:
        raw = load_document(path)
        preprocessed = preprocess_document(raw, self.config.segmenter)
        segments = self.segmenter.segment(preprocessed)
        return segments

    def ingest(self, paths: Iterable[str | Path]) -> None:
        """Ingest a collection of documents into the vector store."""

        all_segments: List[Segment] = []
        for path in paths:
            all_segments.extend(self._ingest_single(path))

        embeddings = self.embedding_generator.embed_segments(all_segments)
        self.vector_store.add_embeddings(embeddings)

        if self.vector_store.config.provider == "memory":
            self.retriever.refresh_lexical_corpus(self.vector_store.documents)

    def retrieve(self, question: str) -> List[RetrievedChunk]:
        """Retrieve relevant chunks for the user question."""

        return self.retriever.retrieve(question)

    def answer(self, question: str) -> PipelineResponse:
        """Return an answer generated by GPT-5 using retrieved context."""

        chunks = self.retrieve(question)
        prompt = build_prompt(question, chunks, self.config.prompt)
        answer = self.llm.generate(prompt, chunks)
        return build_pipeline_response(answer, prompt, chunks)

    def ingest_and_answer(self, paths: Iterable[str | Path], question: str) -> PipelineResponse:
        """Convenience helper to ingest documents and answer a question."""

        self.ingest(paths)
        return self.answer(question)
